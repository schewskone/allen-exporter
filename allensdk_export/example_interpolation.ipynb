{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664ac650-bada-4b99-98a8-4c183230e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6727ecff-df2e-4231-9387-11dc235ade86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "p = !pwd\n",
    "p = os.path.dirname(p[0])\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9c8290-7945-44cb-a54e-24e3f80891dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.experiment import Experiment\n",
    "from experanto.interpolators import Interpolator\n",
    "from experanto.interpolators import ScreenInterpolator\n",
    "\n",
    "root_folder = '../data/example_experiment'\n",
    "\n",
    "e = Experiment(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb5c74-8ee1-4585-8520-453e5e38111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for images at the beginning\n",
    "times = np.arange(300., 320., 0.1)\n",
    "time_steps = len(times)\n",
    "video, valid = e.interpolate(times, device=\"screen\")\n",
    "\n",
    "n_frames, height, width = video.shape\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(video[0], cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "def update(frame):\n",
    "    img.set_array(video[frame])\n",
    "    ax.set_title(f'Time step: {frame}')\n",
    "    return [img]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=n_frames, interval=50, blit=True)\n",
    "\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579dac1-19cc-48d5-bfd2-a075d57f580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for video at the end\n",
    "times = np.arange(4483., 4500., 0.1)\n",
    "time_steps = len(times)\n",
    "video, valid = e.interpolate(times, device=\"screen\")\n",
    "\n",
    "# can also do it using the dataset object using this\n",
    "#video = dataset.__getitem__(4000)['screen']\n",
    "#video = np.squeeze(video, axis=1)\n",
    "\n",
    "n_frames, height, width = video.shape\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(video[0], cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "def update(frame):\n",
    "    img.set_array(video[frame])\n",
    "    ax.set_title(f'Time step: {frame}')\n",
    "    return [img]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=n_frames, interval=50, blit=True)\n",
    "\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6320979-c9a8-4943-953d-e69de8e6577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experanto.datasets import SimpleChunkedDataset, ChunkDataset\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "sampling_rate = 30  # Timestamps generated by this do not match the real ones, why do this? Is this for output data?\n",
    "chunk_size = 20\n",
    "dataset = SimpleChunkedDataset(root_folder, sampling_rate, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "494296bb-376f-40ac-b8e6-e36818b7234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eye_tracker', 'screen', 'treadmill', 'responses', 'timestamps'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 1, 144, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset object and the __getitem__ method can be used to sample data using the interpolators for all modalities at once\n",
    "data = dataset.__getitem__(1000)\n",
    "print(data.keys())\n",
    "data['screen'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "558d6776-1dca-4c79-8e8d-86607e3b1c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChunkDataset(root_folder=root_folder, global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'train',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    }\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "            'eye_tracker': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0,\n",
    "                'transforms': {\n",
    "                    'normalize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "            'treadmill': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0,\n",
    "                'transforms': {\n",
    "                    'normalize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            }\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cc33ad1-e6a5-4043-986e-5fd384b1c4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 144, 144])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_dataset.__getitem__(10)\n",
    "data['screen'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "267c1423-f6e8-4082-87a2-375bbca7e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "data_loaders = OrderedDict()\n",
    "m = 'example_experiment' \n",
    "\n",
    "data_loaders['train'] = OrderedDict()\n",
    "data_loaders['train'][m] =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "it = next(iter(data_loaders['train'][m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7e6243c-b3d6-4d90-acb0-50bab3688039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it['treadmill'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7bb4b93-c73a-4185-8a89-17e08e34cccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eye_tracker', 'screen', 'treadmill', 'responses', 'timestamps'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d3f36-9933-4f6c-a2e9-6cdbc9f6a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
