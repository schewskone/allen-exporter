{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8379a594-a7c8-4af3-9ee5-4e5fbab5a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from experanto.datasets import ChunkDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1953158d-c7ae-4095-9e59-b67d0a19511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '../data/allen_data'\n",
    "sampling_rate = 8  # Timestamps generated by this do not match the real ones, why do this? Is this for output data?\n",
    "chunk_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c6d219-6a13-49eb-8574-aae1248c797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980471', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'train',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    }\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c6d82b-1628-4a67-85f3-cff95acd8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980473', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'val',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    }\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab29670d-fcb5-4d89-9216-8e82652ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "data_loaders = OrderedDict()\n",
    "m = 'allen_data'\n",
    "\n",
    "data_loaders['train'] = OrderedDict()\n",
    "data_loaders['oracle'] = OrderedDict()\n",
    "data_loaders['train'][m] =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "data_loaders['oracle'][m] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc27547e-47be-420f-9aa4-ddd88011bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/src/sensorium_2023/')\n",
    "import torch\n",
    "from sensorium.datasets.mouse_video_loaders import mouse_video_loader\n",
    "from sensorium.utility.scores import get_correlations\n",
    "from nnfabrik.builder import get_trainer\n",
    "from sensorium.models.make_model import make_video_model\n",
    "from nnfabrik.utility.nn_helpers import set_random_seed\n",
    "seed = 42\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16c84c2-52cd-402c-8914-aeaf67f0de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorised_3D_core_dict = dict(\n",
    "    input_channels=3, # With this we can use both rgb and greyscale,  addapt this for me?\n",
    "    hidden_channels=[32, 64, 128],\n",
    "    spatial_input_kernel=(11,11),\n",
    "    temporal_input_kernel=11,\n",
    "    spatial_hidden_kernel=(5,5),\n",
    "    temporal_hidden_kernel=5,\n",
    "    stride=1,\n",
    "    layers=3,\n",
    "    gamma_input_spatial=10,\n",
    "    gamma_input_temporal=0.01, \n",
    "    bias=True, \n",
    "    hidden_nonlinearities='elu', \n",
    "    x_shift=0, \n",
    "    y_shift=0,\n",
    "    batch_norm=True, \n",
    "    laplace_padding=None,\n",
    "    input_regularizer='LaplaceL2norm',\n",
    "    padding=False,\n",
    "    final_nonlin=True,\n",
    "    momentum=0.7\n",
    ")\n",
    "\n",
    "\n",
    "shifter_dict=None\n",
    "\n",
    "\n",
    "readout_dict = dict(\n",
    "    bias=True,\n",
    "    init_mu_range=0.2,\n",
    "    init_sigma=1.0,\n",
    "    gamma_readout=0.0,\n",
    "    gauss_type='full',\n",
    "    grid_mean_predictor=None,\n",
    "    #grid_mean_predictor={\n",
    "    #    'type': 'cortex',\n",
    "    #    'input_dimensions': 2,\n",
    "    #    'hidden_layers': 1,\n",
    "    #    'hidden_features': 30,\n",
    "    #    'final_tanh': True\n",
    "    #},\n",
    "    share_features=False,\n",
    "    share_grid=False,\n",
    "    shared_match_ids=None,\n",
    "    gamma_grid_dispersion=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a450c309-2883-43e8-8fa6-7ba56a866ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The bias is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm_scale is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/readouts/base.py:74: UserWarning: Use of 'gamma_readout' is deprecated. Use 'feature_reg_weight' instead. If 'feature_reg_weight' is defined, 'gamma_readout' is ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "factorised_3d_model = make_video_model(\n",
    "    data_loaders,\n",
    "    seed,\n",
    "    core_dict=factorised_3D_core_dict,\n",
    "    core_type='3D_factorised',\n",
    "    readout_dict=readout_dict.copy(),\n",
    "    readout_type='gaussian',               \n",
    "    use_gru=False,\n",
    "    gru_dict=None,\n",
    "    use_shifter=False,\n",
    "    shifter_dict=shifter_dict,\n",
    "    shifter_type='MLP',\n",
    "    deeplake_ds=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9c16fc-5bf5-46be-bfee-7c1f1b7ad5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoFiringRateEncoder(\n",
       "  (core): Factorized3dCore(\n",
       "    (_input_weight_regularizer): LaplaceL2norm(\n",
       "      (laplace): Laplace()\n",
       "    )\n",
       "    (temporal_regularizer): DepthLaplaceL21d(\n",
       "      (laplace): Laplace1d()\n",
       "    )\n",
       "    (features): Sequential(\n",
       "      (layer0): Sequential(\n",
       "        (conv_spatial): Conv3d(3, 32, kernel_size=(1, 11, 11), stride=(1, 1, 1))\n",
       "        (conv_temporal): Conv3d(32, 32, kernel_size=(11, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(32, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer1): Sequential(\n",
       "        (conv_spatial_1): Conv3d(32, 64, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_1): Conv3d(64, 64, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(64, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (conv_spatial_2): Conv3d(64, 128, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_2): Conv3d(128, 128, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(128, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "    )\n",
       "  ) [Factorized3dCore regularizers: gamma_input_spatial = 10|gamma_input_temporal = 0.01]\n",
       "  \n",
       "  (readout): MultipleFullGaussian2d(\n",
       "    (allen_data): full FullGaussian2d (128 x 126 x 126 -> 32) with bias\n",
       "  )\n",
       "  (nonlinearity_fn): ELU(alpha=1.0)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorised_3d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675e1dcb-014c-428a-8132-2d582feb5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e68d8d4a-76dc-4b0d-a585-7c14a27fec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_fn = \"sensorium.training.video_training_loop.standard_trainer\"\n",
    "\n",
    "trainer_config = {\n",
    "    'dataloaders': data_loaders,  # Keep this as it is (your data loaders)\n",
    "    'seed' : 111,  # Set seed for reproducibility\n",
    "    'use_wandb' : False,  # Disable WandB\n",
    "    'verbose': True,  # Keep verbosity for logging\n",
    "    'lr_decay_steps': 1,  # One decay step (this will not matter for 1 iteration)\n",
    "    'lr_init': 0.005,  # Keep the initial learning rate the same\n",
    "    'device' : device,  # Keep the device (cpu or cuda) unchanged\n",
    "    'detach_core' : False,  # Set False to allow gradients for the core\n",
    "    'deeplake_ds' : False,  # Set to False as you're not using DeepLake\n",
    "    'checkpoint_save_path': '/tmp/',  # Save checkpoints temporarily or disable saving\n",
    "    'max_iter': 1,  # Set max_iter to 1 for a quick test (1 iteration)\n",
    "    'batch_size': 1,  # Small batch size (1) for fast testing\n",
    "}\n",
    "\n",
    "\n",
    "trainer = get_trainer(trainer_fn=trainer_fn, \n",
    "                 trainer_config=trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939376b2-83a6-47ff-b873-13d4dac8f5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optim_step_count = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m validation_score, trainer_output, state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactorised_3d_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/src/sensorium_2023/sensorium/training/video_training_loop.py:180\u001b[0m, in \u001b[0;36mstandard_trainer\u001b[0;34m(model, dataloaders, seed, avg_loss, scale_loss, loss_function, stop_function, loss_accum_batch_n, device, verbose, interval, patience, epoch, lr_init, max_iter, maximize, tolerance, restore_best, lr_decay_steps, lr_decay_factor, min_lr, cb, detach_core, use_wandb, wandb_project, wandb_entity, wandb_name, wandb_model_config, wandb_dataset_config, print_step, save_checkpoints, checkpoint_save_path, chpt_save_step, deeplake_ds, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m batch_no_tot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# train over epochs\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, val_obj \u001b[38;5;129;01min\u001b[39;00m early_stopping(\n\u001b[1;32m    181\u001b[0m     model,\n\u001b[1;32m    182\u001b[0m     stop_closure,\n\u001b[1;32m    183\u001b[0m     interval\u001b[38;5;241m=\u001b[39minterval,\n\u001b[1;32m    184\u001b[0m     patience\u001b[38;5;241m=\u001b[39mpatience,\n\u001b[1;32m    185\u001b[0m     start\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    186\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39mmax_iter,\n\u001b[1;32m    187\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    188\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[1;32m    189\u001b[0m     restore_best\u001b[38;5;241m=\u001b[39mrestore_best,\n\u001b[1;32m    190\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m    191\u001b[0m     lr_decay_steps\u001b[38;5;241m=\u001b[39mlr_decay_steps,\n\u001b[1;32m    192\u001b[0m ):\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# executes callback function if passed in keyword args\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m         cb()\n",
      "File \u001b[0;32m/src/neuralpredictors/neuralpredictors/training/early_stopping.py:127\u001b[0m, in \u001b[0;36mearly_stopping\u001b[0;34m(model, objective, interval, patience, start, max_iter, maximize, tolerance, switch_mode, restore_best, tracker, scheduler, lr_decay_steps, number_warmup_epochs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# turn into a sign\u001b[39;00m\n\u001b[1;32m    126\u001b[0m maximize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maximize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m best_objective \u001b[38;5;241m=\u001b[39m current_objective \u001b[38;5;241m=\u001b[39m \u001b[43m_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m best_state_dict \u001b[38;5;241m=\u001b[39m copy_state(model)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# check if the learning rate scheduler is 'ReduceLROnPlateau' so that we pass the current_objective to step\u001b[39;00m\n",
      "File \u001b[0;32m/src/neuralpredictors/neuralpredictors/training/early_stopping.py:105\u001b[0m, in \u001b[0;36mearly_stopping.<locals>._objective\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m switch_mode:\n\u001b[1;32m    104\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 105\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m switch_mode:\n\u001b[1;32m    107\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(training_status)\n",
      "File \u001b[0;32m/src/sensorium_2023/sensorium/utility/scores.py:80\u001b[0m, in \u001b[0;36mget_correlations\u001b[0;34m(model, dataloaders, tier, device, as_dict, per_neuron, deeplake_ds, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m dl \u001b[38;5;241m=\u001b[39m dataloaders[tier] \u001b[38;5;28;01mif\u001b[39;00m tier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dataloaders\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dl\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 80\u001b[0m     target, output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeeplake_ds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeeplake_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(target, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     88\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m/src/sensorium_2023/sensorium/utility/scores.py:20\u001b[0m, in \u001b[0;36mmodel_predictions\u001b[0;34m(model, dataloader, data_key, device, skip, deeplake_ds)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mcomputes model predictions for a given dataloader and a model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    target: ground truth, i.e. neuronal firing rates of the neurons\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    output: responses as predicted by the network\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m target, output \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     22\u001b[0m     batch_kwargs \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39m_asdict() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m batch\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deeplake_ds:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/src/experanto/experanto/datasets.py:468\u001b[0m, in \u001b[0;36mChunkDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    466\u001b[0m     times \u001b[38;5;241m=\u001b[39m times \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_config[device_name]\u001b[38;5;241m.\u001b[39moffset\n\u001b[1;32m    467\u001b[0m     data, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment\u001b[38;5;241m.\u001b[39minterpolate(times, device\u001b[38;5;241m=\u001b[39mdevice_name)\n\u001b[0;32m--> 468\u001b[0m     out[device_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# remove dim0 for response/eye_tracker/treadmill\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m#phase_shifts = self._experiment.devices[\"responses\"]._phase_shifts\u001b[39;00m\n\u001b[1;32m    471\u001b[0m times_with_phase_shifts \u001b[38;5;241m=\u001b[39m (times \u001b[38;5;241m-\u001b[39m times\u001b[38;5;241m.\u001b[39mmin())[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;66;03m# + phase_shifts[None, :]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py:51\u001b[0m, in \u001b[0;36mCompose.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m needs_unpacking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 51\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/transforms/functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/transforms/_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[1;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:4678\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4676\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[0;32m-> 4678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[1;32m   4680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4681\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[1;32m   4682\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[1;32m   4683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_score, trainer_output, state_dict = trainer(factorised_3d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57268626-dfe3-4b4c-8b53-71fa82d6ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7f0b8-1301-4907-8758-06fe50c47108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
